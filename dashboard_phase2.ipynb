{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b58bfe96",
   "metadata": {},
   "source": [
    "# Apex Weld Quality – Phase 2 Dashboard\n",
    "## Defect Detection (Binary Classification) with Confidence\n",
    "\n",
    "This notebook is the **evaluation dashboard** for Phase 2. It covers:\n",
    "\n",
    "1. **Model & calibration loading** – checkpoint, temperature, threshold\n",
    "2. **Dataset overview** – counts, durations, missing/corrupt stats\n",
    "3. **Label distribution** – defect vs non-defect, defect-type counts\n",
    "4. **Representative examples** – sensor signals, video frame preview, audio waveform/spectrogram\n",
    "5. **Data quality indicators** – class imbalance, outliers, noise\n",
    "6. **Calibrated predictions** – val and test sets\n",
    "7. **Core binary metrics** – accuracy, precision, recall, F1, AUC-ROC, AUC-PR, Brier score\n",
    "8. **Threshold analysis** – sweep plot, operating point\n",
    "9. **Error breakdown** – false positives / false negatives with examples\n",
    "10. **Calibration curve** – reliability diagram\n",
    "11. **Exportable reports** – save all plots/tables, generate summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Imports & Setup ───────────────────────────────────────────\n",
    "import sys, os, json, logging, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    HAS_PLOTLY = True\n",
    "except ImportError:\n",
    "    HAS_PLOTLY = False\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import scipy.signal\n",
    "\n",
    "# Project modules\n",
    "PROJECT_ROOT = Path().resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.config import (\n",
    "    GOOD_WELD_DIR, DEFECT_WELD_DIR, LABELS_CSV, SPLIT_DIR, OUTPUT_DIR,\n",
    "    DASHBOARD_DIR, MODEL_DIR, LABEL_COL, SAMPLE_ID_COL, LABEL_MAP,\n",
    "    SENSOR_COLUMNS, FIXED_SEQ_LEN, IMAGE_SIZE, BATCH_SIZE,\n",
    "    DEFECT_TYPES, CATEGORY_COL, DEFECT_TYPE_COL,\n",
    ")\n",
    "from src.data_ingestion import ingest\n",
    "from src.feature_engineering import build_feature_table, sensor_to_fixed_tensor\n",
    "from src.splitter import load_split\n",
    "from src.dataset import WeldDataset, compute_normalize_stats\n",
    "from src.trainer import WeldClassifier, _get_device\n",
    "from src.calibration import TemperatureScaler, fit_temperature, predict_calibrated\n",
    "from src.evaluation import (\n",
    "    compute_binary_metrics, threshold_sweep, select_threshold,\n",
    "    error_breakdown, full_evaluation_report,\n",
    "    plot_confusion_matrix, plot_roc_and_pr, plot_calibration,\n",
    "    plot_threshold_sweep, plot_error_examples,\n",
    ")\n",
    "\n",
    "# Style\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font_scale=1.05)\n",
    "plt.rcParams.update({\"figure.dpi\": 120, \"savefig.dpi\": 150, \"figure.figsize\": (12, 5)})\n",
    "\n",
    "DASHBOARD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"All imports OK ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88d4bc2",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion & Dataset Overview\n",
    "\n",
    "Ingest all weld runs from `good_weld/` and `defect-weld/`, compute features, and display summary statistics: total runs, durations, sensor row counts, missing/corrupt data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb58106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Ingest all data ───────────────────────────────────────────\n",
    "manifest, sensor_data = ingest(GOOD_WELD_DIR, DEFECT_WELD_DIR)\n",
    "\n",
    "n_good = (manifest[LABEL_COL] == 0).sum()\n",
    "n_defect = (manifest[LABEL_COL] == 1).sum()\n",
    "durations = manifest[\"duration_s\"].dropna()\n",
    "\n",
    "print(f\"{'=' * 55}\")\n",
    "print(f\"  DATASET OVERVIEW\")\n",
    "print(f\"{'=' * 55}\")\n",
    "print(f\"  Total weld runs     : {len(manifest)}\")\n",
    "print(f\"  Labelled good       : {n_good}\")\n",
    "print(f\"  Labelled defect     : {n_defect}\")\n",
    "print(f\"  Sensor channels     : {len(SENSOR_COLUMNS)}  ({', '.join(SENSOR_COLUMNS)})\")\n",
    "print(f\"  Rows per run        : {manifest['n_sensor_rows'].min()}–{manifest['n_sensor_rows'].max()}\"\n",
    "      f\"  (mean={manifest['n_sensor_rows'].mean():.0f})\")\n",
    "print(f\"  Duration (s)        : {durations.min():.1f}–{durations.max():.1f}\"\n",
    "      f\"  (mean={durations.mean():.1f})\")\n",
    "print(f\"  Images per run      : {manifest['n_images'].min()}–{manifest['n_images'].max()}\")\n",
    "n_issues = manifest[\"issues\"].apply(len).gt(0).sum()\n",
    "print(f\"  Runs with issues    : {n_issues} / {len(manifest)}\")\n",
    "print(f\"{'=' * 55}\")\n",
    "\n",
    "# Overview plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 9))\n",
    "fig.suptitle(\"Dataset Overview\", fontsize=15, fontweight=\"bold\")\n",
    "\n",
    "axes[0, 0].hist(durations, bins=20, color=\"steelblue\", edgecolor=\"white\")\n",
    "axes[0, 0].axvline(durations.mean(), color=\"red\", ls=\"--\", label=f\"mean={durations.mean():.1f}s\")\n",
    "axes[0, 0].set_xlabel(\"Duration (s)\"); axes[0, 0].set_ylabel(\"Count\")\n",
    "axes[0, 0].set_title(\"Weld Duration Distribution\"); axes[0, 0].legend()\n",
    "\n",
    "axes[0, 1].hist(manifest[\"n_sensor_rows\"], bins=20, color=\"darkorange\", edgecolor=\"white\")\n",
    "axes[0, 1].set_xlabel(\"Rows\"); axes[0, 1].set_ylabel(\"Count\")\n",
    "axes[0, 1].set_title(\"Sensor Readings per Run\")\n",
    "\n",
    "# Missing data heatmap\n",
    "miss_data = []\n",
    "for _, row in manifest.iterrows():\n",
    "    sid = row[SAMPLE_ID_COL]\n",
    "    sdf = sensor_data[sid]\n",
    "    nan_pct = sdf[SENSOR_COLUMNS].isnull().mean() * 100\n",
    "    miss_data.append(nan_pct.values)\n",
    "miss_arr = np.array(miss_data)\n",
    "axes[1, 0].imshow(miss_arr.T, aspect=\"auto\", cmap=\"Reds\", interpolation=\"nearest\")\n",
    "axes[1, 0].set_yticks(range(len(SENSOR_COLUMNS)))\n",
    "axes[1, 0].set_yticklabels(SENSOR_COLUMNS, fontsize=8)\n",
    "axes[1, 0].set_xlabel(\"Run index\"); axes[1, 0].set_title(\"Missing Data (% NaN per channel)\")\n",
    "\n",
    "# Global sensor stats\n",
    "all_sensor = pd.concat([sensor_data[sid][SENSOR_COLUMNS] for sid in sensor_data], ignore_index=True)\n",
    "stats = all_sensor.describe().T[[\"mean\", \"std\", \"min\", \"max\"]].round(2)\n",
    "axes[1, 1].axis(\"off\")\n",
    "tbl = axes[1, 1].table(cellText=stats.values, rowLabels=stats.index, colLabels=stats.columns,\n",
    "                         loc=\"center\", cellLoc=\"center\")\n",
    "tbl.auto_set_font_size(False); tbl.set_fontsize(9); tbl.scale(1.2, 1.4)\n",
    "axes[1, 1].set_title(\"Global Sensor Statistics\", fontsize=12, pad=20)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "fig.savefig(DASHBOARD_DIR / \"p2_01_dataset_overview.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87384d5",
   "metadata": {},
   "source": [
    "## 2. Label Distribution – Defect vs Non-Defect & Defect Sub-Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e372ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Label distribution ────────────────────────────────────────\n",
    "labelled = manifest[manifest[LABEL_COL].notna()].copy()\n",
    "labelled[\"label_name\"] = labelled[LABEL_COL].map(LABEL_MAP)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# (a) Binary: good vs defect\n",
    "vc = labelled[\"label_name\"].value_counts()\n",
    "colors_bin = [\"#4CAF50\" if x == \"good\" else \"#F44336\" for x in vc.index]\n",
    "axes[0].bar(vc.index, vc.values, color=colors_bin, edgecolor=\"white\", width=0.5)\n",
    "for i, (lbl, cnt) in enumerate(vc.items()):\n",
    "    axes[0].text(i, cnt + 3, str(cnt), ha=\"center\", fontweight=\"bold\")\n",
    "axes[0].set_title(\"Good vs Defect\")\n",
    "axes[0].set_ylabel(\"# Runs\")\n",
    "\n",
    "# (b) Pie chart\n",
    "axes[1].pie(vc.values, labels=vc.index, autopct=\"%1.0f%%\", colors=colors_bin,\n",
    "            startangle=90, textprops={\"fontsize\": 12})\n",
    "axes[1].set_title(\"Label Proportions\")\n",
    "\n",
    "# (c) Defect sub-types\n",
    "if DEFECT_TYPE_COL in manifest.columns:\n",
    "    defect_only = manifest[manifest[LABEL_COL] == 1]\n",
    "    dt_counts = defect_only[DEFECT_TYPE_COL].value_counts()\n",
    "    palette = sns.color_palette(\"Set2\", len(dt_counts))\n",
    "    axes[2].barh(dt_counts.index, dt_counts.values, color=palette, edgecolor=\"white\")\n",
    "    for i, (dtype, cnt) in enumerate(dt_counts.items()):\n",
    "        axes[2].text(cnt + 1, i, str(cnt), va=\"center\", fontweight=\"bold\")\n",
    "    axes[2].set_xlabel(\"# Runs\")\n",
    "    axes[2].set_title(\"Defect Sub-Type Counts\")\n",
    "    axes[2].invert_yaxis()\n",
    "else:\n",
    "    axes[2].text(0.5, 0.5, \"No defect_type column\", transform=axes[2].transAxes,\n",
    "                 ha=\"center\", va=\"center\")\n",
    "    axes[2].set_title(\"Defect Sub-Types (N/A)\")\n",
    "\n",
    "plt.suptitle(\"Label Distribution Analysis\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "fig.savefig(DASHBOARD_DIR / \"p2_02_label_distribution.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Class imbalance assessment\n",
    "majority, minority = vc.max(), vc.min()\n",
    "ratio = majority / minority if minority > 0 else float(\"inf\")\n",
    "print(f\"\\nClass imbalance ratio: {ratio:.1f}:1  (majority={vc.idxmax()}, minority={vc.idxmin()})\")\n",
    "if ratio > 3:\n",
    "    print(\"⚠ Significant imbalance – using weighted loss + stratified splits\")\n",
    "else:\n",
    "    print(\"✓ Imbalance within acceptable range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b489dd",
   "metadata": {},
   "source": [
    "## 3. Representative Examples – Sensor Signals, Video Frame, Audio Waveform\n",
    "\n",
    "For selected good and defect runs, we visualise:\n",
    "- **6-channel sensor time-series** with weld-phase annotations\n",
    "- **Video frame preview** (first frame extracted from `.avi`)\n",
    "- **Audio waveform & spectrogram** (from `.flac`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd31966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Helper: locate run directory from manifest ───────────────\n",
    "def _find_run_dir(sid, manifest_df):\n",
    "    \"\"\"Locate the run directory for a sample_id by searching known bases.\"\"\"\n",
    "    row = manifest_df[manifest_df[SAMPLE_ID_COL] == sid].iloc[0]\n",
    "    cat = row[CATEGORY_COL]\n",
    "    lbl = row[LABEL_COL]\n",
    "    base = GOOD_WELD_DIR if lbl == 0 else DEFECT_WELD_DIR\n",
    "    run_dir = base / cat / sid\n",
    "    if run_dir.exists():\n",
    "        return run_dir\n",
    "    # Fallback: search\n",
    "    for p in base.rglob(sid):\n",
    "        if p.is_dir():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# Pick example runs: 1 good, 1 defect per sub-type (up to 4 total)\n",
    "example_ids = []\n",
    "good_ids = manifest[manifest[LABEL_COL] == 0][SAMPLE_ID_COL].tolist()\n",
    "if good_ids:\n",
    "    example_ids.append(good_ids[len(good_ids)//2])  # middle good sample\n",
    "\n",
    "if DEFECT_TYPE_COL in manifest.columns:\n",
    "    for dtype in sorted(manifest[manifest[LABEL_COL] == 1][DEFECT_TYPE_COL].unique())[:3]:\n",
    "        subset = manifest[(manifest[DEFECT_TYPE_COL] == dtype)]\n",
    "        if len(subset) > 0:\n",
    "            example_ids.append(subset[SAMPLE_ID_COL].iloc[len(subset)//2])\n",
    "else:\n",
    "    defect_ids = manifest[manifest[LABEL_COL] == 1][SAMPLE_ID_COL].tolist()\n",
    "    if defect_ids:\n",
    "        example_ids.append(defect_ids[0])\n",
    "\n",
    "print(f\"Plotting {len(example_ids)} representative runs\\n\")\n",
    "\n",
    "for sid in example_ids:\n",
    "    run_dir = _find_run_dir(sid, manifest)\n",
    "    sdf = sensor_data[sid].copy()\n",
    "    lbl_val = manifest[manifest[SAMPLE_ID_COL] == sid][LABEL_COL].values[0]\n",
    "    lbl_name = LABEL_MAP.get(int(lbl_val), \"?\")\n",
    "    dtype_name = \"\"\n",
    "    if DEFECT_TYPE_COL in manifest.columns:\n",
    "        dtype_name = manifest[manifest[SAMPLE_ID_COL] == sid][DEFECT_TYPE_COL].values[0]\n",
    "\n",
    "    # Time axis\n",
    "    if \"datetime\" in sdf.columns and sdf[\"datetime\"].notna().any():\n",
    "        t = (sdf[\"datetime\"] - sdf[\"datetime\"].min()).dt.total_seconds().values\n",
    "    else:\n",
    "        t = np.arange(len(sdf)) * 0.11\n",
    "\n",
    "    # ── Build figure: 4 rows (sensors top 3 rows, media bottom row) ──\n",
    "    fig = plt.figure(figsize=(18, 16))\n",
    "    gs = gridspec.GridSpec(4, 3, figure=fig, height_ratios=[1, 1, 1, 1.2], hspace=0.35, wspace=0.3)\n",
    "    title = f\"Run: {sid}  |  {lbl_name.upper()}\"\n",
    "    if dtype_name and dtype_name != \"good\":\n",
    "        title += f\"  ({dtype_name})\"\n",
    "    fig.suptitle(title, fontsize=15, fontweight=\"bold\")\n",
    "\n",
    "    # Sensor signals (6 channels, 3×2 grid in top 3 rows)\n",
    "    for i, col in enumerate(SENSOR_COLUMNS):\n",
    "        row_i, col_i = i // 2, i % 2\n",
    "        ax = fig.add_subplot(gs[row_i, col_i])\n",
    "        ax.plot(t, sdf[col].values, linewidth=0.8, color=\"steelblue\")\n",
    "        ax.set_ylabel(col, fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        if row_i == 2:\n",
    "            ax.set_xlabel(\"Time (s)\")\n",
    "\n",
    "        # Phase annotations\n",
    "        current = sdf[\"Primary Weld Current\"].values\n",
    "        arc_on = np.where(current > 10.0)[0]\n",
    "        if len(arc_on) > 0:\n",
    "            t_start, t_end = t[arc_on[0]], t[arc_on[-1]]\n",
    "            ax.axvspan(0, t_start, alpha=0.06, color=\"blue\")\n",
    "            ax.axvspan(t_end, t[-1], alpha=0.06, color=\"gray\")\n",
    "\n",
    "    # ── Image preview (top-right area) ──\n",
    "    ax_img = fig.add_subplot(gs[0, 2])\n",
    "    if run_dir:\n",
    "        img_dir = run_dir / \"images\"\n",
    "        imgs = sorted(img_dir.glob(\"*.jpg\")) + sorted(img_dir.glob(\"*.png\")) if img_dir.exists() else []\n",
    "        if imgs:\n",
    "            img = Image.open(imgs[len(imgs)//2]).convert(\"RGB\")\n",
    "            ax_img.imshow(img)\n",
    "            ax_img.set_title(f\"Inspection Photo ({len(imgs)} total)\", fontsize=10)\n",
    "        else:\n",
    "            ax_img.text(0.5, 0.5, \"No images\", ha=\"center\", va=\"center\", transform=ax_img.transAxes)\n",
    "    ax_img.axis(\"off\")\n",
    "\n",
    "    # ── Video frame preview ──\n",
    "    ax_vid = fig.add_subplot(gs[1, 2])\n",
    "    avi_path = run_dir / f\"{sid}.avi\" if run_dir else None\n",
    "    if avi_path and avi_path.exists():\n",
    "        try:\n",
    "            import cv2\n",
    "            cap = cv2.VideoCapture(str(avi_path))\n",
    "            # Read frame at 25% into the video\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames // 4)\n",
    "            ret, frame = cap.read()\n",
    "            cap.release()\n",
    "            if ret:\n",
    "                ax_vid.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                ax_vid.set_title(f\"Video Frame ({total_frames} frames)\", fontsize=10)\n",
    "            else:\n",
    "                ax_vid.text(0.5, 0.5, \"Could not read frame\", ha=\"center\", va=\"center\",\n",
    "                           transform=ax_vid.transAxes)\n",
    "        except ImportError:\n",
    "            ax_vid.text(0.5, 0.5, f\"Video: {avi_path.name}\\n(cv2 not installed)\", ha=\"center\",\n",
    "                       va=\"center\", transform=ax_vid.transAxes, fontsize=9)\n",
    "    else:\n",
    "        ax_vid.text(0.5, 0.5, \"No .avi file\", ha=\"center\", va=\"center\", transform=ax_vid.transAxes)\n",
    "    ax_vid.axis(\"off\")\n",
    "\n",
    "    # ── Audio waveform + spectrogram ──\n",
    "    ax_wave = fig.add_subplot(gs[2, 2])\n",
    "    ax_spec = fig.add_subplot(gs[3, :])\n",
    "\n",
    "    flac_path = run_dir / f\"{sid}.flac\" if run_dir else None\n",
    "    audio_loaded = False\n",
    "    if flac_path and flac_path.exists():\n",
    "        try:\n",
    "            import soundfile as sf\n",
    "            audio, sr = sf.read(str(flac_path))\n",
    "            audio_loaded = True\n",
    "        except ImportError:\n",
    "            pass\n",
    "        if not audio_loaded:\n",
    "            try:\n",
    "                from scipy.io import wavfile as _wf\n",
    "                # FLAC may not work with scipy; try anyway\n",
    "                sr, audio = _wf.read(str(flac_path))\n",
    "                audio = audio.astype(np.float32) / max(np.abs(audio).max(), 1)\n",
    "                audio_loaded = True\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    if audio_loaded:\n",
    "        if audio.ndim > 1:\n",
    "            audio = audio.mean(axis=1)\n",
    "        t_audio = np.arange(len(audio)) / sr\n",
    "\n",
    "        # Waveform\n",
    "        ax_wave.plot(t_audio, audio, linewidth=0.3, color=\"purple\")\n",
    "        ax_wave.set_ylabel(\"Amplitude\")\n",
    "        ax_wave.set_title(f\"Audio Waveform (sr={sr} Hz, {len(audio)/sr:.1f}s)\", fontsize=10)\n",
    "        ax_wave.grid(True, alpha=0.3)\n",
    "\n",
    "        # Spectrogram\n",
    "        nperseg = min(1024, len(audio) // 4)\n",
    "        f_spec, t_spec, Sxx = scipy.signal.spectrogram(audio, fs=sr, nperseg=nperseg)\n",
    "        ax_spec.pcolormesh(t_spec, f_spec, 10 * np.log10(Sxx + 1e-10), shading=\"gouraud\", cmap=\"magma\")\n",
    "        ax_spec.set_ylabel(\"Frequency (Hz)\")\n",
    "        ax_spec.set_xlabel(\"Time (s)\")\n",
    "        ax_spec.set_title(\"Audio Spectrogram\", fontsize=11)\n",
    "    else:\n",
    "        if flac_path and flac_path.exists():\n",
    "            ax_wave.text(0.5, 0.5, f\"Audio: {flac_path.name}\\n(no audio library to decode FLAC)\",\n",
    "                        ha=\"center\", va=\"center\", transform=ax_wave.transAxes, fontsize=9)\n",
    "        else:\n",
    "            ax_wave.text(0.5, 0.5, \"No .flac file\", ha=\"center\", va=\"center\", transform=ax_wave.transAxes)\n",
    "        ax_wave.axis(\"off\")\n",
    "        ax_spec.text(0.5, 0.5, \"Spectrogram unavailable\", ha=\"center\", va=\"center\",\n",
    "                    transform=ax_spec.transAxes)\n",
    "        ax_spec.axis(\"off\")\n",
    "\n",
    "    fig.savefig(DASHBOARD_DIR / f\"p2_03_example_{sid}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282d10cd",
   "metadata": {},
   "source": [
    "## 4. Data Quality Indicators – Class Imbalance, Outliers, Noise\n",
    "\n",
    "Per-run sensor statistics, signal-to-noise ratio during arcing, IQR-based outlier detection, and constant-column analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c458c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Per-run quality stats ─────────────────────────────────────\n",
    "run_stats = []\n",
    "for sid, sdf in sensor_data.items():\n",
    "    row = {\"sample_id\": sid}\n",
    "    lbl_rows = manifest[manifest[SAMPLE_ID_COL] == sid]\n",
    "    row[\"label\"] = int(lbl_rows[LABEL_COL].values[0]) if len(lbl_rows) else -1\n",
    "\n",
    "    for col in SENSOR_COLUMNS:\n",
    "        vals = sdf[col].dropna()\n",
    "        row[f\"{col}__mean\"] = vals.mean() if len(vals) else 0\n",
    "        row[f\"{col}__std\"] = vals.std() if len(vals) else 0\n",
    "\n",
    "    current = sdf[\"Primary Weld Current\"].values\n",
    "    arcing = current > 10.0\n",
    "    if arcing.sum() > 5:\n",
    "        arc_current = current[arcing]\n",
    "        row[\"arc_current_snr\"] = arc_current.mean() / arc_current.std() if arc_current.std() > 0 else np.inf\n",
    "    else:\n",
    "        row[\"arc_current_snr\"] = 0.0\n",
    "    run_stats.append(row)\n",
    "\n",
    "run_stats_df = pd.DataFrame(run_stats).set_index(\"sample_id\")\n",
    "\n",
    "# ── Boxplots by class ────────────────────────────────────────\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 9))\n",
    "fig.suptitle(\"Per-Run Sensor Statistics – Good vs Defect\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "for i, col in enumerate(SENSOR_COLUMNS):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    good_vals = run_stats_df[run_stats_df[\"label\"] == 0][f\"{col}__mean\"].dropna()\n",
    "    defect_vals = run_stats_df[run_stats_df[\"label\"] == 1][f\"{col}__mean\"].dropna()\n",
    "    bp = ax.boxplot([good_vals, defect_vals], labels=[\"Good\", \"Defect\"], patch_artist=True)\n",
    "    bp[\"boxes\"][0].set_facecolor(\"#4CAF50\"); bp[\"boxes\"][0].set_alpha(0.6)\n",
    "    bp[\"boxes\"][1].set_facecolor(\"#F44336\"); bp[\"boxes\"][1].set_alpha(0.6)\n",
    "    ax.set_title(f\"{col} (mean)\", fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "fig.savefig(DASHBOARD_DIR / \"p2_04_quality_boxplots.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# ── Outlier detection ─────────────────────────────────────────\n",
    "print(\"\\n=== IQR Outlier Runs per Sensor ===\")\n",
    "outlier_sids = set()\n",
    "for col in SENSOR_COLUMNS:\n",
    "    means = run_stats_df[f\"{col}__mean\"].dropna()\n",
    "    Q1, Q3 = means.quantile(0.25), means.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = (means < Q1 - 1.5*IQR) | (means > Q3 + 1.5*IQR)\n",
    "    n_out = mask.sum()\n",
    "    outlier_sids.update(means[mask].index.tolist())\n",
    "    print(f\"  {col:30s}: {n_out} outlier runs\")\n",
    "\n",
    "print(f\"\\n  Total unique outlier runs: {len(outlier_sids)} / {len(run_stats_df)}\")\n",
    "\n",
    "# SNR summary\n",
    "snr = run_stats_df[\"arc_current_snr\"].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "print(f\"\\n  Arc Current SNR: mean={snr.mean():.2f}  median={snr.median():.2f}  \"\n",
    "      f\"min={snr.min():.2f}  max={snr.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce18101d",
   "metadata": {},
   "source": [
    "## 5. Load Model, Calibrate, and Generate Predictions\n",
    "\n",
    "Load the trained checkpoint, apply temperature scaling on the validation set, select the optimal threshold, and produce calibrated predictions for both val and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79bfde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Build features, datasets, loaders ─────────────────────────\n",
    "feature_df = build_feature_table(manifest, sensor_data)\n",
    "split_map = load_split()\n",
    "\n",
    "# Norm stats from training set\n",
    "norm_stats = compute_normalize_stats(sensor_data, split_map[\"train\"])\n",
    "\n",
    "datasets, loaders = {}, {}\n",
    "for split_name, ids in split_map.items():\n",
    "    ds = WeldDataset(\n",
    "        manifest=manifest, sensor_data=sensor_data,\n",
    "        sample_ids=ids, feature_df=feature_df, normalize_stats=norm_stats,\n",
    "    )\n",
    "    datasets[split_name] = ds\n",
    "    loaders[split_name] = DataLoader(\n",
    "        ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True,\n",
    "    )\n",
    "\n",
    "n_features = datasets[\"train\"][0][\"features\"].shape[0]\n",
    "n_channels = len(SENSOR_COLUMNS)\n",
    "\n",
    "for k, v in split_map.items():\n",
    "    print(f\"  {k:5s}: {len(v)} samples\")\n",
    "print(f\"  Feature vector size: {n_features}\")\n",
    "\n",
    "# ── Load model ────────────────────────────────────────────────\n",
    "device = _get_device()\n",
    "model_path = MODEL_DIR / \"weld_classifier.pt\"\n",
    "model = WeldClassifier(n_channels, n_features).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "print(f\"\\n✓ Model loaded from {model_path}\")\n",
    "\n",
    "# ── Temperature scaling ──────────────────────────────────────\n",
    "scaler = fit_temperature(model, loaders[\"val\"], device=device)\n",
    "T = scaler.temperature.item()\n",
    "print(f\"✓ Learned temperature: T = {T:.4f}\")\n",
    "\n",
    "# ── Generate calibrated predictions ──────────────────────────\n",
    "pred_dfs = {}\n",
    "for split_name in [\"val\", \"test\"]:\n",
    "    results = predict_calibrated(scaler, loaders[split_name], device=device)\n",
    "    rows = []\n",
    "    for sid, info in results.items():\n",
    "        rows.append({\n",
    "            \"sample_id\": sid,\n",
    "            \"label\": info[\"label\"],\n",
    "            \"p_defect\": round(info[\"p_defect\"], 6),\n",
    "            \"p_good\": round(info[\"p_good\"], 6),\n",
    "            \"confidence\": round(info[\"confidence\"], 6),\n",
    "        })\n",
    "    pred_dfs[split_name] = pd.DataFrame(rows)\n",
    "    print(f\"  {split_name}: {len(pred_dfs[split_name])} predictions\")\n",
    "\n",
    "# ── Threshold selection (on val ONLY) ────────────────────────\n",
    "val_labelled = pred_dfs[\"val\"][pred_dfs[\"val\"][\"label\"].notna()].copy()\n",
    "threshold = select_threshold(\n",
    "    val_labelled[\"label\"].values.astype(int),\n",
    "    val_labelled[\"p_defect\"].values,\n",
    "    strategy=\"f1\",\n",
    ")\n",
    "print(f\"\\n✓ Optimal threshold (F1 on val): {threshold:.4f}\")\n",
    "\n",
    "# Apply threshold\n",
    "for split_name, df in pred_dfs.items():\n",
    "    df[\"pred_defect\"] = (df[\"p_defect\"] >= threshold).astype(int)\n",
    "    df[\"confidence\"] = df.apply(\n",
    "        lambda r: r[\"p_defect\"] if r[\"pred_defect\"] == 1 else (1 - r[\"p_defect\"]), axis=1\n",
    "    ).round(6)\n",
    "\n",
    "# Save predictions_binary.csv\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "pred_dfs[\"test\"].to_csv(OUTPUT_DIR / \"predictions_binary.csv\", index=False)\n",
    "pred_dfs[\"val\"].to_csv(OUTPUT_DIR / \"predictions_binary_val.csv\", index=False)\n",
    "print(f\"✓ Predictions saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b369a32",
   "metadata": {},
   "source": [
    "## 6. Threshold Analysis – Metric Sweep\n",
    "\n",
    "How do precision, recall, F1, and accuracy change as the decision threshold varies? The red dashed line marks the threshold chosen by maximising F1 on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9552b0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Threshold sweep on validation set ─────────────────────────\n",
    "sweep_df = threshold_sweep(\n",
    "    val_labelled[\"label\"].values.astype(int),\n",
    "    val_labelled[\"p_defect\"].values,\n",
    ")\n",
    "plot_threshold_sweep(sweep_df, threshold, save_path=DASHBOARD_DIR / \"p2_05_threshold_sweep.png\")\n",
    "plt.show()\n",
    "\n",
    "# Show metrics at the chosen threshold\n",
    "m_val = compute_binary_metrics(\n",
    "    val_labelled[\"label\"].values.astype(int),\n",
    "    val_labelled[\"p_defect\"].values,\n",
    "    threshold=threshold,\n",
    ")\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"  VALIDATION METRICS @ threshold={threshold:.4f}\")\n",
    "print(f\"{'=' * 50}\")\n",
    "for k in [\"accuracy\", \"precision\", \"recall\", \"specificity\", \"f1\", \"roc_auc\", \"avg_precision\", \"brier_score\"]:\n",
    "    print(f\"  {k:20s}: {m_val[k]:.4f}\")\n",
    "print(f\"  TP={m_val['tp']}  FP={m_val['fp']}  FN={m_val['fn']}  TN={m_val['tn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f277c",
   "metadata": {},
   "source": [
    "## 7. Test Set Evaluation – Core Binary Metrics\n",
    "\n",
    "The threshold was fixed on the validation set. These are unseen test-set results with **no threshold tuning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd0367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Full evaluation on TEST set ───────────────────────────────\n",
    "test_labelled = pred_dfs[\"test\"][pred_dfs[\"test\"][\"label\"].notna()].copy()\n",
    "report = full_evaluation_report(\n",
    "    predictions_df=test_labelled,\n",
    "    threshold=threshold,\n",
    "    split_name=\"test\",\n",
    "    save_dir=DASHBOARD_DIR,\n",
    ")\n",
    "print(report[\"report_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf43fc",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrix – Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c2b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Confusion matrix (already saved by full_evaluation_report) ─\n",
    "y_true = test_labelled[\"label\"].values.astype(int)\n",
    "y_prob = test_labelled[\"p_defect\"].values\n",
    "y_pred = (y_prob >= threshold).astype(int)\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, save_path=DASHBOARD_DIR / \"p2_06_confusion_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b799e624",
   "metadata": {},
   "source": [
    "## 9. ROC Curve & Precision-Recall Curve – Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfc8ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_and_pr(y_true, y_prob, threshold=threshold,\n",
    "               save_path=DASHBOARD_DIR / \"p2_07_roc_pr.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9216e8",
   "metadata": {},
   "source": [
    "## 10. Calibration Analysis – Reliability Diagram\n",
    "\n",
    "A well-calibrated model's predicted probabilities match the true fraction of positives. Temperature scaling helps close any gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce293c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calibration(y_true, y_prob, n_bins=10, label=\"Temp-scaled\",\n",
    "                save_path=DASHBOARD_DIR / \"p2_08_calibration.png\")\n",
    "plt.show()\n",
    "\n",
    "# Brier score context\n",
    "brier = report[\"metrics\"][\"brier_score\"]\n",
    "print(f\"Brier Score: {brier:.4f}  (0 = perfect, 0.25 = random)\")\n",
    "if brier < 0.1:\n",
    "    print(\"✓ Excellent calibration\")\n",
    "elif brier < 0.2:\n",
    "    print(\"○ Reasonable calibration\")\n",
    "else:\n",
    "    print(\"⚠ Poor calibration – consider re-calibration or different model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa8fb10",
   "metadata": {},
   "source": [
    "## 11. Error Breakdown – False Positives & False Negatives\n",
    "\n",
    "Which samples did the model get wrong? The bar charts show the worst misclassifications ranked by predicted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5796476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Error breakdown ───────────────────────────────────────────\n",
    "fp_df, fn_df = error_breakdown(test_labelled, threshold=threshold)\n",
    "\n",
    "plot_error_examples(fp_df, fn_df, top_n=10,\n",
    "                   save_path=DASHBOARD_DIR / \"p2_09_error_examples.png\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFalse Positives: {len(fp_df)}  (good runs wrongly flagged as defect)\")\n",
    "if len(fp_df) > 0:\n",
    "    display(fp_df[[\"sample_id\", \"label\", \"p_defect\", \"confidence\"]].head(10))\n",
    "\n",
    "print(f\"\\nFalse Negatives: {len(fn_df)}  (defect runs missed by the model)\")\n",
    "if len(fn_df) > 0:\n",
    "    display(fn_df[[\"sample_id\", \"label\", \"p_defect\", \"confidence\"]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f1c65e",
   "metadata": {},
   "source": [
    "## 12. Exportable Reports & Artefacts\n",
    "\n",
    "Save a consolidated metrics CSV and a plain-text report that can be attached to pull-requests or audit logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eded974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Export artefacts ──────────────────────────────────────────\n",
    "# 1) Save a single-row metrics CSV for programmatic comparison\n",
    "metrics_csv = DASHBOARD_DIR / \"phase2_metrics_summary.csv\"\n",
    "pd.DataFrame([test_metrics]).to_csv(metrics_csv, index=False)\n",
    "print(f\"[✓] Metrics summary saved → {metrics_csv}\")\n",
    "\n",
    "# 2) Save the full text report next to the plots\n",
    "report_path = DASHBOARD_DIR / \"phase2_report.txt\"\n",
    "report_lines = [\n",
    "    \"=\" * 60,\n",
    "    \" Phase 2 – Binary Classification Evaluation Report\",\n",
    "    \"=\" * 60,\n",
    "    f\"\\nModel          : WeldClassifier (SensorCNN + FeatureMLP)\",\n",
    "    f\"Threshold      : {threshold:.4f}  (strategy: F1-optimal on val set)\",\n",
    "    f\"Temperature    : {temperature:.4f}\",\n",
    "    f\"Test samples   : {len(test_labelled)}\",\n",
    "    \"\",\n",
    "    \"── Key Metrics ──────────────────────────────────────────\",\n",
    "    f\"  Accuracy     : {test_metrics['accuracy']:.4f}\",\n",
    "    f\"  Precision    : {test_metrics['precision']:.4f}\",\n",
    "    f\"  Recall       : {test_metrics['recall']:.4f}\",\n",
    "    f\"  F1 Score     : {test_metrics['f1']:.4f}\",\n",
    "    f\"  Specificity  : {test_metrics['specificity']:.4f}\",\n",
    "    f\"  ROC AUC      : {test_metrics['roc_auc']:.4f}\",\n",
    "    f\"  PR AUC (AP)  : {test_metrics['avg_precision']:.4f}\",\n",
    "    f\"  Brier Score  : {test_metrics['brier_score']:.4f}\",\n",
    "    f\"  Log Loss     : {test_metrics['log_loss']:.4f}\",\n",
    "    \"\",\n",
    "    \"── Confusion Matrix (rows=actual, cols=predicted) ──────\",\n",
    "    f\"  TN={test_metrics['tn']}  FP={test_metrics['fp']}\",\n",
    "    f\"  FN={test_metrics['fn']}  TP={test_metrics['tp']}\",\n",
    "    \"\",\n",
    "    \"── Error Summary ────────────────────────────────────────\",\n",
    "    f\"  False Positives : {len(fp_df)}  (good → flagged)\",\n",
    "    f\"  False Negatives : {len(fn_df)}  (defect → missed)\",\n",
    "    \"\",\n",
    "    \"── Saved Artefacts ──────────────────────────────────────\",\n",
    "]\n",
    "\n",
    "# List all saved plots\n",
    "for p in sorted(DASHBOARD_DIR.glob(\"p2_*.png\")):\n",
    "    report_lines.append(f\"  {p.name}\")\n",
    "report_lines.append(f\"  phase2_metrics_summary.csv\")\n",
    "report_lines.append(f\"  phase2_report.txt\")\n",
    "report_lines.append(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "report_text = \"\\n\".join(report_lines)\n",
    "report_path.write_text(report_text, encoding=\"utf-8\")\n",
    "print(f\"[✓] Full report saved → {report_path}\")\n",
    "print(\"\\n\" + report_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f0f306",
   "metadata": {},
   "source": [
    "## 13. Phase 2 – Data Card\n",
    "\n",
    "| Field | Value |\n",
    "|---|---|\n",
    "| **Task** | Binary weld defect detection (good vs defect) |\n",
    "| **Model** | `WeldClassifier` — dual-branch: 1-D SensorCNN (6 channels) + Feature MLP (104 dims) → 192-d fusion → 2-class logits |\n",
    "| **Calibration** | Temperature scaling (Guo et al., ICML 2017) fitted on validation set |\n",
    "| **Threshold** | F1-optimal on validation set, saved in `threshold.json` |\n",
    "| **Confidence** | `max(p_defect, 1 − p_defect)` after temperature calibration |\n",
    "| **Output** | `predictions_binary.csv` — columns: `sample_id, p_defect, pred_defect, confidence, label` |\n",
    "| **Dataset** | 1 552 weld runs (good_weld + defect-weld), 70 / 15 / 15 % train / val / test split |\n",
    "| **Input per run** | 6-channel sensor time-series (padded to 400 steps) + 104 engineered features |\n",
    "| **Sensor columns** | current, voltage, wire_feed_speed, gas_flow_rate, heat_input, energy |\n",
    "| **Normalisation** | Per-channel z-score computed on training set (`normalize_stats.json`) |\n",
    "| **Artefacts** | `best_model.pt`, `temperature.json`, `threshold.json`, `normalize_stats.json`, `predictions_binary.csv` |\n",
    "| **Limitations** | Trained on GTAW butt joints only; may not generalise to other joint types or welding processes |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
